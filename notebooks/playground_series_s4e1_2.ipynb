{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255aafef-661c-4a19-b25f-feda4e1dc461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d5b5f-60b7-46a6-a7a2-476c68d2f8e1",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45370fd8-bec0-4316-90d5-47b3a7470089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    train_url = os.path.join('../data/playground_series_s4e1', \n",
    "                         'train.csv')\n",
    "    test_url = os.path.join('../data/playground_series_s4e1', \n",
    "                            'test.csv')\n",
    "    origin_url = os.path.join('../data/playground_series_s4e1', \n",
    "                              'Churn_Modelling.csv')\n",
    "    if dataset == 'train':\n",
    "        df = pd.read_csv(train_url)\n",
    "    elif dataset == 'test':\n",
    "        df = pd.read_csv(test_url)\n",
    "    elif dataset == 'origin':\n",
    "        df = pd.read_csv(origin_url)\n",
    "    else:\n",
    "        raise ValueError(f'{dataset} is not a supported dataset')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3febde-9e10-4955-8620-fc538c484208",
   "metadata": {},
   "source": [
    "### Target Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af5ff70-ac55-4e2b-8db2-1ce694654def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_target(df, col):\n",
    "    # calcualted the mean exited rate by specified columns\n",
    "    df_target = df.groupby(col).agg({'exited': 'mean'})\n",
    "    df_target = df_target.reset_index()\n",
    "    df_target = df_target.rename(columns={'exited': col+'_target'})\n",
    "    \n",
    "    df = pd.merge(df, df_target, on=col, how='left', \n",
    "                  validate='m:1')\n",
    "    df = df.drop(columns=[col])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b34058f-95af-4135-8100-eb993b8d9bd0",
   "metadata": {},
   "source": [
    "### Baseline Models\n",
    "Accuracy: roc_auc\n",
    "1. logistic regression\n",
    "2. catboost classifier\n",
    "3. xgboost classifier\n",
    "4. lightgbm classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aeb43cc-a8ea-4b60-9242-738693b6b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(df, model='lr', scaler='minmax'):\n",
    "    \n",
    "    X_train = df.drop(columns=['exited'])\n",
    "    y_train = df['exited']\n",
    "    \n",
    "    # stadardization\n",
    "    if scaler == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        raise ValueError(f'{scaler} is not supported')\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # cross validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    if model == 'lr':\n",
    "        model = LogisticRegression()\n",
    "    elif model == 'cat':\n",
    "        model = CatBoostClassifier(verbose=0)\n",
    "    elif model == 'xgb':\n",
    "        model = XGBClassifier(verbosity=0)\n",
    "    elif model == 'lgb': \n",
    "        model = LGBMClassifier(verbose=-1)\n",
    "    else:\n",
    "        model = model\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring='roc_auc')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edc9c435-03f7-4aca-a6a4-3c65e3ba7d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score of lr with minmax is  0.8353.\n",
      "The average score of lr with standard is  0.8353.\n",
      "The average score of cat with minmax is  0.8996.\n",
      "The average score of cat with standard is  0.8995.\n",
      "The average score of xgb with minmax is  0.8981.\n",
      "The average score of xgb with standard is  0.8982.\n",
      "The average score of lgb with minmax is  0.9000.\n",
      "The average score of lgb with standard is  0.8998.\n",
      "CPU times: total: 32min 23s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the data\n",
    "train_df = load_data('train')\n",
    "train_df.columns = train_df.columns.str.lower()\n",
    "train_df = train_df.drop(columns=['id', 'customerid'])\n",
    "\n",
    "# impute the dataset\n",
    "cols = ['surname', 'geography', 'gender', 'hascrcard', 'isactivemember']\n",
    "for col in cols:\n",
    "    train_df = impute_target(train_df, col)\n",
    "    \n",
    "# calculate the roc scores\n",
    "for model in ['lr', 'cat', 'xgb', 'lgb']:\n",
    "    for scaler in ['minmax', 'standard']: \n",
    "        scores = calculate_score(train_df, model=model, scaler=scaler)\n",
    "        print(f'The average score of {model} with {scaler} is {scores.mean(): .4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cc48b-e21c-4dcd-910c-026d4adf1332",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10fcadd5-b4be-458e-811f-1c46796f8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from optuna.integration import LightGBMPruningCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2a4f16e-9448-445c-b365-5516e6218878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, y):\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "        # tree structure\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12, step=1),\n",
    "        'max_leaves': trial.suggest_int('max_leaves', 20, 3000, step=20), \n",
    "\n",
    "        # better accuracy\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.9, step=0.01),\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [10000]),\n",
    "\n",
    "        # combat overfitting\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.99, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 0.99, log=True),\n",
    "        'subsample_freq': trial.suggest_categorical('subsample_freq', [1]), \n",
    "        'reg_alpha': trial.suggest_categorical('reg_alpha', [0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0]), # L1 regularization\n",
    "        'reg_lambda': trial.suggest_categorical('reg_lambda', [0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0]), # L2 regularization\n",
    "\n",
    "        'random_state': trial.suggest_categorical('random_state', [42]), \n",
    "        'n_jobs': trial.suggest_categorical('n_jobs', [-1]), \n",
    "        'early_stopping_rounds': trial.suggest_categorical('early_stopping_rounds', [100]), \n",
    "        'eval_metric': trial.suggest_categorical('eval_metric', ['auc']), \n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        model = XGBClassifier(objective='binary:logistic', **param_grid)\n",
    "        model.fit(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            eval_set=[(X_test, y_test)], \n",
    "            #eval_metric='auc', \n",
    "            #early_stopping_rounds=100,\n",
    "            #callbacks=[LightGBMPruningCallback(trial, 'auc'), ], \n",
    "            verbose=0, \n",
    "        )\n",
    "        y_preds = model.predict_proba(X_test)[:, 1]\n",
    "        cv_scores[idx] = roc_auc_score(y_test, y_preds)\n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15062134-1809-4d0d-9a15-6e87df106a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-01-04 14:28:01,090]\u001b[0m A new study created in memory with name: XGB Classifier\u001b[0m\n",
      "\u001b[32m[I 2024-01-04 14:29:19,867]\u001b[0m Trial 0 finished with value: 0.8971765313887399 and parameters: {'max_depth': 12, 'max_leaves': 680, 'learning_rate': 0.86, 'n_estimators': 10000, 'colsample_bytree': 0.7755750129863637, 'subsample': 0.3133886566213953, 'subsample_freq': 1, 'reg_alpha': 100.0, 'reg_lambda': 0.1, 'random_state': 42, 'n_jobs': -1, 'early_stopping_rounds': 100, 'eval_metric': 'auc'}. Best is trial 0 with value: 0.8971765313887399.\u001b[0m\n",
      "\u001b[32m[I 2024-01-04 14:29:36,685]\u001b[0m Trial 1 finished with value: 0.8971499114327066 and parameters: {'max_depth': 4, 'max_leaves': 1300, 'learning_rate': 0.54, 'n_estimators': 10000, 'colsample_bytree': 0.4224457325155376, 'subsample': 0.21665061696578028, 'subsample_freq': 1, 'reg_alpha': 0.1, 'reg_lambda': 10.0, 'random_state': 42, 'n_jobs': -1, 'early_stopping_rounds': 100, 'eval_metric': 'auc'}. Best is trial 0 with value: 0.8971765313887399.\u001b[0m\n",
      "\u001b[32m[I 2024-01-04 14:31:02,134]\u001b[0m Trial 2 finished with value: 0.8997686605928097 and parameters: {'max_depth': 7, 'max_leaves': 2100, 'learning_rate': 0.42000000000000004, 'n_estimators': 10000, 'colsample_bytree': 0.32013804454690464, 'subsample': 0.7219138990080689, 'subsample_freq': 1, 'reg_alpha': 50.0, 'reg_lambda': 0.1, 'random_state': 42, 'n_jobs': -1, 'early_stopping_rounds': 100, 'eval_metric': 'auc'}. Best is trial 2 with value: 0.8997686605928097.\u001b[0m\n",
      "\u001b[32m[I 2024-01-04 14:31:37,217]\u001b[0m Trial 3 finished with value: 0.8968203367257377 and parameters: {'max_depth': 10, 'max_leaves': 760, 'learning_rate': 0.77, 'n_estimators': 10000, 'colsample_bytree': 0.6690497600887179, 'subsample': 0.8183176096041722, 'subsample_freq': 1, 'reg_alpha': 10.0, 'reg_lambda': 5.0, 'random_state': 42, 'n_jobs': -1, 'early_stopping_rounds': 100, 'eval_metric': 'auc'}. Best is trial 2 with value: 0.8997686605928097.\u001b[0m\n",
      "\u001b[32m[I 2024-01-04 14:32:43,273]\u001b[0m Trial 4 finished with value: 0.8992807050032499 and parameters: {'max_depth': 8, 'max_leaves': 1780, 'learning_rate': 0.64, 'n_estimators': 10000, 'colsample_bytree': 0.20084065054416544, 'subsample': 0.9404710620436294, 'subsample_freq': 1, 'reg_alpha': 50.0, 'reg_lambda': 0.1, 'random_state': 42, 'n_jobs': -1, 'early_stopping_rounds': 100, 'eval_metric': 'auc'}. Best is trial 2 with value: 0.8997686605928097.\u001b[0m\n",
      "\u001b[32m[I 2024-01-04 14:33:06,759]\u001b[0m Trial 5 finished with value: 0.8943143902532963 and parameters: {'max_depth': 10, 'max_leaves': 480, 'learning_rate': 0.72, 'n_estimators': 10000, 'colsample_bytree': 0.2488769137507074, 'subsample': 0.905356382458151, 'subsample_freq': 1, 'reg_alpha': 0.1, 'reg_lambda': 5.0, 'random_state': 42, 'n_jobs': -1, 'early_stopping_rounds': 100, 'eval_metric': 'auc'}. Best is trial 2 with value: 0.8997686605928097.\u001b[0m\n",
      "\u001b[32m[I 2024-01-04 14:33:42,330]\u001b[0m Trial 6 finished with value: 0.8989884537057786 and parameters: {'max_depth': 10, 'max_leaves': 920, 'learning_rate': 0.21000000000000002, 'n_estimators': 10000, 'colsample_bytree': 0.6245672606193988, 'subsample': 0.3327680615629185, 'subsample_freq': 1, 'reg_alpha': 0.1, 'reg_lambda': 50.0, 'random_state': 42, 'n_jobs': -1, 'early_stopping_rounds': 100, 'eval_metric': 'auc'}. Best is trial 2 with value: 0.8997686605928097.\u001b[0m\n",
      "\u001b[32m[I 2024-01-04 14:34:02,680]\u001b[0m Trial 7 finished with value: 0.893434387408346 and parameters: {'max_depth': 10, 'max_leaves': 980, 'learning_rate': 0.75, 'n_estimators': 10000, 'colsample_bytree': 0.21500353899487645, 'subsample': 0.7794800526138655, 'subsample_freq': 1, 'reg_alpha': 0.1, 'reg_lambda': 5.0, 'random_state': 42, 'n_jobs': -1, 'early_stopping_rounds': 100, 'eval_metric': 'auc'}. Best is trial 2 with value: 0.8997686605928097.\u001b[0m\n",
      "\u001b[32m[I 2024-01-04 14:34:17,200]\u001b[0m Trial 8 finished with value: 0.8900959336569174 and parameters: {'max_depth': 5, 'max_leaves': 1800, 'learning_rate': 0.89, 'n_estimators': 10000, 'colsample_bytree': 0.31207768664283253, 'subsample': 0.20351503860925216, 'subsample_freq': 1, 'reg_alpha': 1.0, 'reg_lambda': 1.0, 'random_state': 42, 'n_jobs': -1, 'early_stopping_rounds': 100, 'eval_metric': 'auc'}. Best is trial 2 with value: 0.8997686605928097.\u001b[0m\n",
      "\u001b[32m[I 2024-01-04 14:36:14,614]\u001b[0m Trial 9 finished with value: 0.899753981630156 and parameters: {'max_depth': 9, 'max_leaves': 2340, 'learning_rate': 0.17, 'n_estimators': 10000, 'colsample_bytree': 0.5889362087299407, 'subsample': 0.4968436832986494, 'subsample_freq': 1, 'reg_alpha': 50.0, 'reg_lambda': 0.1, 'random_state': 42, 'n_jobs': -1, 'early_stopping_rounds': 100, 'eval_metric': 'auc'}. Best is trial 2 with value: 0.8997686605928097.\u001b[0m\n",
      "\u001b[32m[I 2024-01-04 14:36:48,831]\u001b[0m Trial 10 finished with value: 0.8993930479219312 and parameters: {'max_depth': 6, 'max_leaves': 2220, 'learning_rate': 0.39, 'n_estimators': 10000, 'colsample_bytree': 0.40573295008237054, 'subsample': 0.5679200891521593, 'subsample_freq': 1, 'reg_alpha': 0.5, 'reg_lambda': 100.0, 'random_state': 42, 'n_jobs': -1, 'early_stopping_rounds': 100, 'eval_metric': 'auc'}. Best is trial 2 with value: 0.8997686605928097.\u001b[0m\n",
      "\u001b[32m[I 2024-01-04 14:45:09,192]\u001b[0m Trial 11 finished with value: 0.8999632387301529 and parameters: {'max_depth': 7, 'max_leaves': 2800, 'learning_rate': 0.02, 'n_estimators': 10000, 'colsample_bytree': 0.5004469995535239, 'subsample': 0.5620868014038862, 'subsample_freq': 1, 'reg_alpha': 50.0, 'reg_lambda': 0.1, 'random_state': 42, 'n_jobs': -1, 'early_stopping_rounds': 100, 'eval_metric': 'auc'}. Best is trial 11 with value: 0.8999632387301529.\u001b[0m\n",
      "\u001b[33m[W 2024-01-04 15:04:59,287]\u001b[0m Trial 12 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<timed exec>\", line 8, in <lambda>\n",
      "  File \"C:\\Users\\Fan\\AppData\\Local\\Temp\\ipykernel_37276\\3126321432.py\", line 33, in objective\n",
      "    model.fit(\n",
      "  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\core.py\", line 575, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\sklearn.py\", line 1400, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\core.py\", line 575, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\core.py\", line 1778, in update\n",
      "    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:9\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\optuna\\study\\study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\optuna\\study\\_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    230\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    233\u001b[0m ):\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[1;32m<timed exec>:8\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, X, y)\u001b[0m\n\u001b[0;32m     30\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m y[train_idx], y[test_idx]\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam_grid)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#eval_metric='auc', \u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#early_stopping_rounds=100,\u001b[39;49;00m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#callbacks=[LightGBMPruningCallback(trial, 'auc'), ], \u001b[39;49;00m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m y_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     43\u001b[0m cv_scores[idx] \u001b[38;5;241m=\u001b[39m roc_auc_score(y_test, y_preds)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    574\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\sklearn.py:1400\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1379\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[0;32m   1380\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1381\u001b[0m )\n\u001b[0;32m   1382\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1383\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1384\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1397\u001b[0m     enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_categorical,\n\u001b[0;32m   1398\u001b[0m )\n\u001b[1;32m-> 1400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    574\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\core.py:1778\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_features(dtrain)\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1779\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = train_df.drop(columns=['exited'])\n",
    "y_train = train_df['exited']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', study_name='XGB Classifier')\n",
    "func = lambda trial: objective(trial, X_train_scaled, y_train)\n",
    "study.optimize(func, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6ef6c8c-6836-4714-a74a-e3b5f1a92547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest value： 0.89755\n"
     ]
    }
   ],
   "source": [
    "print(f'\\tBest value： {study.best_value:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2c4a6c8-b305-42f6-b9bb-4930816a78c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\core.py\", line 575, in inner_f\n    return f(**kwargs)\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\sklearn.py\", line 1400, in fit\n    self._Booster = train(\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\core.py\", line 575, in inner_f\n    return f(**kwargs)\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\training.py\", line 182, in train\n    if cb_container.after_iteration(bst, i, dtrain, evals):\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\callback.py\", line 246, in after_iteration\n    ret = any(c.after_iteration(model, epoch, self.history)\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\callback.py\", line 246, in <genexpr>\n    ret = any(c.after_iteration(model, epoch, self.history)\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\callback.py\", line 411, in after_iteration\n    assert len(evals_log.keys()) >= 1, msg\nAssertionError: Must have at least 1 validation dataset for early stopping.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstudy\u001b[38;5;241m.\u001b[39mbest_params)\n\u001b[1;32m----> 2\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m scores\u001b[38;5;241m.\u001b[39mmean()\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mcalculate_score\u001b[1;34m(df, model, scaler)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m---> 27\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroc_auc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:285\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[1;32m--> 285\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\core.py\", line 575, in inner_f\n    return f(**kwargs)\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\sklearn.py\", line 1400, in fit\n    self._Booster = train(\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\core.py\", line 575, in inner_f\n    return f(**kwargs)\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\training.py\", line 182, in train\n    if cb_container.after_iteration(bst, i, dtrain, evals):\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\callback.py\", line 246, in after_iteration\n    ret = any(c.after_iteration(model, epoch, self.history)\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\callback.py\", line 246, in <genexpr>\n    ret = any(c.after_iteration(model, epoch, self.history)\n  File \"C:\\Users\\Fan\\Anaconda3\\envs\\gradientboosting\\lib\\site-packages\\xgboost\\callback.py\", line 411, in after_iteration\n    assert len(evals_log.keys()) >= 1, msg\nAssertionError: Must have at least 1 validation dataset for early stopping.\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(objective='binary:logistic', **study.best_params)\n",
    "scores = calculate_score(train_df, model)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65a856-ea2c-41ca-94d5-8b4743ce9bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GradientBoostingDecisionTrees",
   "language": "python",
   "name": "gradientboosting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
